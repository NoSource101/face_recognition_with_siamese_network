{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network with Triplet Loss Function for Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesncn/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "#import PIL\n",
    "#from PIL import Image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, subtract, concatenate, Lambda, add, maximum\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import load_model, model_from_json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(inputs, dist='euclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "    loss = positive_distance - negative_distance\n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, 2 + loss)\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(1 + K.exp(loss))\n",
    "        \n",
    "    returned_loss = K.mean(loss)\n",
    "    return returned_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distances(inputs, dist='euclidean', margin='maxplus'):\n",
    "    anchor, positive, negative = inputs\n",
    "    positive_distance = K.square(anchor - positive)\n",
    "    negative_distance = K.square(anchor - negative)\n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "\n",
    "    return positive_distance, negative_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encoding_network to make predictions based on trained network\n",
    "\n",
    "json_file = open('/home/cesncn/Desktop/github_projects/face_recognition/code/saved_model/siamese_network_arch.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "siamese_network = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "siamese_network.load_weights('/home/cesncn/Desktop/github_projects/face_recognition/code/saved_model/siamese_model_weights.h5')\n",
    "\n",
    "json_file = open('/home/cesncn/Desktop/github_projects/face_recognition/code/saved_model/encoding_network_arch.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "encoding_network = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#encoding_network.load_weights('saved_model/encoding_network_weights.h5')\n",
    "weights = siamese_network.get_layer('model_1').get_weights()\n",
    "encoding_network.get_layer('model_1').set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn3d_branch2c: (Should be same)\n",
      " [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"bn3d_branch2c: (Should be same)\\n\", \n",
    "      siamese_network.get_layer('model_1').get_layer('bn3d_branch2c').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn3d_branch2c: (Should be same)\n",
      " [array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"bn3d_branch2c: (Should be same)\\n\", \n",
    "      encoding_network.get_layer('model_1').get_layer('bn3d_branch2c').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.18826778 0.09931731 0.         0.19902033 0.\n",
      "  0.         0.07259142 0.         0.03435858 0.00204392 0.\n",
      "  0.         0.         0.         0.         0.         0.07314678\n",
      "  0.         0.1956443  0.10277999 0.11595597 0.         0.\n",
      "  0.08969536 0.         0.01977132 0.08038978 0.16505438 0.\n",
      "  0.         0.         0.         0.26760548 0.         0.01886074\n",
      "  0.         0.15270634 0.         0.         0.         0.\n",
      "  0.         0.         0.01888521 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.15825288 0.07595807\n",
      "  0.         0.02682878 0.18796109 0.15299928 0.         0.20337489\n",
      "  0.304782   0.04112374 0.         0.         0.         0.13826758\n",
      "  0.         0.         0.         0.06589665 0.         0.\n",
      "  0.12547688 0.         0.         0.         0.11451785 0.\n",
      "  0.11566667 0.         0.         0.07059661 0.         0.\n",
      "  0.21494782 0.         0.         0.         0.         0.\n",
      "  0.         0.01610852 0.11646396 0.         0.         0.09127239\n",
      "  0.06730396 0.         0.         0.3122447  0.01537459 0.\n",
      "  0.         0.18354364 0.         0.01887049 0.07274951 0.\n",
      "  0.         0.20037691 0.         0.         0.         0.\n",
      "  0.         0.20998539 0.         0.         0.08952683 0.21027894\n",
      "  0.         0.04971442 0.         0.0358188  0.0844241  0.18919894\n",
      "  0.11487304 0.        ]]\n",
      "0.8110099\n",
      "1.0241592\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.04871384 0.06294005 0.04519922 0.23924208 0.\n",
      "  0.         0.09972595 0.         0.20536558 0.04766675 0.\n",
      "  0.         0.         0.         0.         0.08230005 0.\n",
      "  0.0134393  0.06120107 0.00825851 0.184851   0.17827809 0.\n",
      "  0.12874839 0.         0.07922365 0.16944464 0.20192622 0.04506346\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27617207 0.         0.         0.         0.03628879\n",
      "  0.         0.         0.17632723 0.07941461 0.         0.\n",
      "  0.         0.         0.         0.09646956 0.         0.10675461\n",
      "  0.         0.09513668 0.35575208 0.0265954  0.         0.27763095\n",
      "  0.1274679  0.         0.         0.         0.         0.01718415\n",
      "  0.05350178 0.01784769 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0434895  0.         0.07804462 0.0346333  0.         0.19044565\n",
      "  0.18360665 0.         0.00654614 0.09108515 0.         0.\n",
      "  0.1169253  0.         0.01605561 0.         0.         0.07681773\n",
      "  0.03809457 0.07490345 0.         0.21077485 0.18368894 0.\n",
      "  0.         0.17812757 0.         0.01941088 0.         0.\n",
      "  0.         0.20801556 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18067822 0.\n",
      "  0.         0.08386625 0.         0.08635926 0.13901727 0.07545551\n",
      "  0.         0.        ]]\n",
      "0.8327045\n",
      "0.92947435\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.02313179 0.         0.04968265 0.         0.\n",
      "  0.         0.00636031 0.         0.         0.         0.\n",
      "  0.034229   0.         0.         0.         0.         0.16919747\n",
      "  0.         0.         0.         0.32854903 0.17487612 0.\n",
      "  0.11803889 0.         0.10524967 0.24881148 0.06069982 0.\n",
      "  0.         0.         0.         0.05795809 0.         0.09875994\n",
      "  0.09718165 0.2307521  0.         0.         0.         0.37963304\n",
      "  0.03525729 0.07299147 0.04662742 0.07800429 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.0406415\n",
      "  0.         0.         0.1899197  0.         0.         0.12158538\n",
      "  0.12577884 0.12083587 0.         0.         0.         0.13735394\n",
      "  0.         0.04554015 0.07196285 0.         0.00298722 0.\n",
      "  0.08784847 0.         0.         0.         0.         0.14021938\n",
      "  0.         0.0320268  0.07791223 0.08355968 0.         0.02863704\n",
      "  0.1466349  0.         0.         0.03770104 0.         0.\n",
      "  0.         0.00847463 0.03635358 0.01843922 0.         0.1736219\n",
      "  0.         0.         0.141037   0.10458347 0.12568    0.\n",
      "  0.         0.12685153 0.         0.08107968 0.21305023 0.01307491\n",
      "  0.09021291 0.18354692 0.18522148 0.11024334 0.         0.\n",
      "  0.07762468 0.07962698 0.1140164  0.00214763 0.14917554 0.06246227\n",
      "  0.         0.02580301 0.02901649 0.10602479 0.         0.17241955\n",
      "  0.         0.        ]]\n",
      "0.91567546\n",
      "0.9744184\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.08987249 0.         0.03070119 0.1768714  0.\n",
      "  0.         0.23009084 0.         0.14046697 0.07285628 0.\n",
      "  0.05590405 0.         0.         0.         0.         0.\n",
      "  0.         0.23182069 0.         0.18908818 0.13607123 0.\n",
      "  0.22047016 0.02299133 0.22002527 0.24567892 0.1936717  0.\n",
      "  0.         0.24855149 0.         0.08739811 0.         0.00591612\n",
      "  0.         0.12965688 0.         0.         0.         0.04613055\n",
      "  0.         0.         0.0054569  0.01055048 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14399378\n",
      "  0.         0.00902676 0.3958388  0.         0.         0.\n",
      "  0.04502255 0.         0.         0.01886963 0.         0.01163838\n",
      "  0.05150678 0.         0.05503293 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02899388 0.\n",
      "  0.         0.         0.         0.         0.         0.00577786\n",
      "  0.05800198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09400591 0.         0.         0.05333159\n",
      "  0.14758675 0.04092914 0.         0.22962908 0.0328159  0.\n",
      "  0.         0.         0.         0.         0.257535   0.\n",
      "  0.         0.2388096  0.         0.         0.         0.\n",
      "  0.         0.18180287 0.         0.         0.16894631 0.\n",
      "  0.         0.         0.         0.         0.00618189 0.1438291\n",
      "  0.04553578 0.        ]]\n",
      "0.78261584\n",
      "0.91975623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.15688182 0.         0.1464214  0.25773114 0.\n",
      "  0.         0.10962376 0.         0.         0.3116729  0.\n",
      "  0.         0.         0.00085027 0.         0.         0.03191967\n",
      "  0.         0.14288945 0.08107249 0.04948526 0.03385342 0.\n",
      "  0.07551229 0.         0.         0.         0.12977754 0.\n",
      "  0.         0.         0.         0.16284476 0.         0.12933274\n",
      "  0.         0.13280039 0.         0.         0.         0.\n",
      "  0.         0.00851787 0.0859881  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08950116 0.0432783\n",
      "  0.         0.         0.19009638 0.07994725 0.         0.08206249\n",
      "  0.28073975 0.03087867 0.         0.         0.         0.0125942\n",
      "  0.02755334 0.16213861 0.00563625 0.         0.         0.14765847\n",
      "  0.10156435 0.         0.         0.         0.12351096 0.\n",
      "  0.02676183 0.00318279 0.         0.         0.         0.0306547\n",
      "  0.13671738 0.         0.         0.15595852 0.         0.\n",
      "  0.         0.         0.         0.13913824 0.         0.\n",
      "  0.13888472 0.         0.06897181 0.         0.13874583 0.\n",
      "  0.         0.21329668 0.         0.         0.         0.\n",
      "  0.         0.26472926 0.         0.20058663 0.         0.0383453\n",
      "  0.         0.3011632  0.         0.09672365 0.2064432  0.\n",
      "  0.         0.03651284 0.03847203 0.08492775 0.         0.03798102\n",
      "  0.13882561 0.06564884]]\n",
      "0.8704643\n",
      "0.9629477\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.13804758 0.02666431 0.0885588  0.32634953 0.\n",
      "  0.         0.05481295 0.         0.10387585 0.23455003 0.\n",
      "  0.05834958 0.         0.         0.04851351 0.         0.\n",
      "  0.         0.12951617 0.00825818 0.         0.04848238 0.04477582\n",
      "  0.13536547 0.         0.0513872  0.22242866 0.2590026  0.\n",
      "  0.         0.14416303 0.         0.0357344  0.         0.\n",
      "  0.         0.1308323  0.         0.         0.         0.18747142\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02133594 0.16449083\n",
      "  0.         0.09419238 0.18338591 0.11364026 0.         0.1741046\n",
      "  0.07966033 0.1569227  0.         0.         0.         0.\n",
      "  0.04332511 0.         0.         0.         0.         0.\n",
      "  0.08277349 0.         0.         0.         0.14359558 0.\n",
      "  0.09526524 0.         0.         0.         0.         0.\n",
      "  0.06462887 0.         0.00472938 0.         0.         0.\n",
      "  0.         0.10617433 0.06306624 0.         0.         0.17345192\n",
      "  0.24985893 0.         0.         0.00372503 0.         0.02905266\n",
      "  0.         0.08625388 0.09713244 0.         0.21223159 0.\n",
      "  0.10024024 0.23404068 0.         0.         0.         0.\n",
      "  0.         0.15957598 0.         0.12999384 0.         0.0429341\n",
      "  0.         0.09562372 0.         0.         0.16098055 0.20978472\n",
      "  0.11315563 0.        ]]\n",
      "0.74566996\n",
      "1.0940032\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.2537487  0.         0.20325361 0.1265731  0.\n",
      "  0.         0.06974641 0.         0.         0.2729104  0.\n",
      "  0.0544199  0.04579989 0.03020638 0.         0.         0.14317673\n",
      "  0.         0.18137379 0.         0.10376646 0.00736924 0.\n",
      "  0.11890767 0.         0.         0.         0.06908192 0.\n",
      "  0.         0.         0.         0.         0.         0.14338562\n",
      "  0.         0.10573988 0.03799438 0.         0.         0.03707119\n",
      "  0.         0.05110149 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12072238 0.11243647 0.14920989\n",
      "  0.         0.06518451 0.2482838  0.         0.         0.03321321\n",
      "  0.21314621 0.14198157 0.         0.         0.         0.\n",
      "  0.         0.1057435  0.         0.         0.00595904 0.07535785\n",
      "  0.25204018 0.         0.         0.         0.23489463 0.\n",
      "  0.         0.         0.         0.05603129 0.         0.\n",
      "  0.22747062 0.         0.         0.02863556 0.00781704 0.\n",
      "  0.         0.12154954 0.01067476 0.1316815  0.         0.\n",
      "  0.18966478 0.         0.09558509 0.04752787 0.         0.04835446\n",
      "  0.         0.11993122 0.13816981 0.06021277 0.19793408 0.\n",
      "  0.         0.13145857 0.         0.15884659 0.         0.\n",
      "  0.00575729 0.22591922 0.         0.12429902 0.13360582 0.\n",
      "  0.         0.         0.         0.         0.06676734 0.0005757\n",
      "  0.09692299 0.0169274 ]]\n",
      "0.9032989\n",
      "0.9224956\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.11241665 0.15769427 0.00432189 0.17423011 0.\n",
      "  0.         0.         0.         0.         0.10505838 0.\n",
      "  0.         0.21046427 0.09701475 0.         0.12332974 0.\n",
      "  0.         0.31542954 0.04465519 0.         0.03952006 0.16495323\n",
      "  0.         0.         0.         0.24479876 0.03873619 0.07531048\n",
      "  0.         0.         0.         0.04248306 0.         0.06203818\n",
      "  0.         0.11862684 0.         0.         0.         0.2175964\n",
      "  0.05254328 0.08939525 0.18369247 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13079277 0.         0.         0.01046467\n",
      "  0.11750672 0.2902821  0.         0.         0.         0.0932264\n",
      "  0.         0.12935832 0.         0.0264788  0.         0.\n",
      "  0.09895001 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02535605 0.         0.\n",
      "  0.11671134 0.06756288 0.         0.13939834 0.00278368 0.\n",
      "  0.10101946 0.         0.10624422 0.         0.04610511 0.\n",
      "  0.07433914 0.         0.13792883 0.         0.02866771 0.\n",
      "  0.         0.25896454 0.10305006 0.02367982 0.10460515 0.\n",
      "  0.         0.21632147 0.         0.         0.         0.11936771\n",
      "  0.         0.23971468 0.         0.         0.12383686 0.11072952\n",
      "  0.         0.1322495  0.         0.04328636 0.01617576 0.22328988\n",
      "  0.         0.        ]]\n",
      "0.8358626\n",
      "0.9447954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.05359045 0.         0.19971271 0.09293785 0.\n",
      "  0.         0.17619899 0.         0.12276229 0.01158989 0.\n",
      "  0.1642689  0.         0.         0.         0.         0.02052508\n",
      "  0.         0.1712485  0.         0.1259787  0.10864962 0.00673577\n",
      "  0.13313371 0.         0.         0.19761382 0.16877887 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15067199 0.         0.01751545 0.         0.06416465\n",
      "  0.         0.01008259 0.11089277 0.         0.         0.\n",
      "  0.00260819 0.         0.         0.         0.07039156 0.11669619\n",
      "  0.         0.         0.0198301  0.07534957 0.         0.10581107\n",
      "  0.10826094 0.23587193 0.         0.         0.         0.14474519\n",
      "  0.03990385 0.24569774 0.05332239 0.         0.         0.02503274\n",
      "  0.06924947 0.         0.         0.         0.16375738 0.\n",
      "  0.         0.         0.10483756 0.03896459 0.         0.00103872\n",
      "  0.12617768 0.         0.         0.15033664 0.07608878 0.\n",
      "  0.         0.         0.04173323 0.         0.         0.02027378\n",
      "  0.         0.         0.         0.20448172 0.15321477 0.\n",
      "  0.         0.24219424 0.         0.         0.01853731 0.\n",
      "  0.         0.18942395 0.05272972 0.         0.         0.\n",
      "  0.         0.16129577 0.05647558 0.         0.22768675 0.15824004\n",
      "  0.         0.13292734 0.17059036 0.10030711 0.         0.23751172\n",
      "  0.17510988 0.        ]]\n",
      "0.8511891\n",
      "0.89945596\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.18826778 0.09931731 0.         0.19902033 0.\n",
      "  0.         0.07259142 0.         0.03435858 0.00204392 0.\n",
      "  0.         0.         0.         0.         0.         0.07314678\n",
      "  0.         0.1956443  0.10277999 0.11595597 0.         0.\n",
      "  0.08969536 0.         0.01977132 0.08038978 0.16505438 0.\n",
      "  0.         0.         0.         0.26760548 0.         0.01886074\n",
      "  0.         0.15270634 0.         0.         0.         0.\n",
      "  0.         0.         0.01888521 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.15825288 0.07595807\n",
      "  0.         0.02682878 0.18796109 0.15299928 0.         0.20337489\n",
      "  0.304782   0.04112374 0.         0.         0.         0.13826758\n",
      "  0.         0.         0.         0.06589665 0.         0.\n",
      "  0.12547688 0.         0.         0.         0.11451785 0.\n",
      "  0.11566667 0.         0.         0.07059661 0.         0.\n",
      "  0.21494782 0.         0.         0.         0.         0.\n",
      "  0.         0.01610852 0.11646396 0.         0.         0.09127239\n",
      "  0.06730396 0.         0.         0.3122447  0.01537459 0.\n",
      "  0.         0.18354364 0.         0.01887049 0.07274951 0.\n",
      "  0.         0.20037691 0.         0.         0.         0.\n",
      "  0.         0.20998539 0.         0.         0.08952683 0.21027894\n",
      "  0.         0.04971442 0.         0.0358188  0.0844241  0.18919894\n",
      "  0.11487304 0.        ]]\n",
      "0.8110099\n",
      "0.89260846\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.04871384 0.06294005 0.04519922 0.23924208 0.\n",
      "  0.         0.09972595 0.         0.20536558 0.04766675 0.\n",
      "  0.         0.         0.         0.         0.08230005 0.\n",
      "  0.0134393  0.06120107 0.00825851 0.184851   0.17827809 0.\n",
      "  0.12874839 0.         0.07922365 0.16944464 0.20192622 0.04506346\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27617207 0.         0.         0.         0.03628879\n",
      "  0.         0.         0.17632723 0.07941461 0.         0.\n",
      "  0.         0.         0.         0.09646956 0.         0.10675461\n",
      "  0.         0.09513668 0.35575208 0.0265954  0.         0.27763095\n",
      "  0.1274679  0.         0.         0.         0.         0.01718415\n",
      "  0.05350178 0.01784769 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.0434895  0.         0.07804462 0.0346333  0.         0.19044565\n",
      "  0.18360665 0.         0.00654614 0.09108515 0.         0.\n",
      "  0.1169253  0.         0.01605561 0.         0.         0.07681773\n",
      "  0.03809457 0.07490345 0.         0.21077485 0.18368894 0.\n",
      "  0.         0.17812757 0.         0.01941088 0.         0.\n",
      "  0.         0.20801556 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.18067822 0.\n",
      "  0.         0.08386625 0.         0.08635926 0.13901727 0.07545551\n",
      "  0.         0.        ]]\n",
      "0.8327045\n",
      "0.8191191\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.02313179 0.         0.04968265 0.         0.\n",
      "  0.         0.00636031 0.         0.         0.         0.\n",
      "  0.034229   0.         0.         0.         0.         0.16919747\n",
      "  0.         0.         0.         0.32854903 0.17487612 0.\n",
      "  0.11803889 0.         0.10524967 0.24881148 0.06069982 0.\n",
      "  0.         0.         0.         0.05795809 0.         0.09875994\n",
      "  0.09718165 0.2307521  0.         0.         0.         0.37963304\n",
      "  0.03525729 0.07299147 0.04662742 0.07800429 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.0406415\n",
      "  0.         0.         0.1899197  0.         0.         0.12158538\n",
      "  0.12577884 0.12083587 0.         0.         0.         0.13735394\n",
      "  0.         0.04554015 0.07196285 0.         0.00298722 0.\n",
      "  0.08784847 0.         0.         0.         0.         0.14021938\n",
      "  0.         0.0320268  0.07791223 0.08355968 0.         0.02863704\n",
      "  0.1466349  0.         0.         0.03770104 0.         0.\n",
      "  0.         0.00847463 0.03635358 0.01843922 0.         0.1736219\n",
      "  0.         0.         0.141037   0.10458347 0.12568    0.\n",
      "  0.         0.12685153 0.         0.08107968 0.21305023 0.01307491\n",
      "  0.09021291 0.18354692 0.18522148 0.11024334 0.         0.\n",
      "  0.07762468 0.07962698 0.1140164  0.00214763 0.14917554 0.06246227\n",
      "  0.         0.02580301 0.02901649 0.10602479 0.         0.17241955\n",
      "  0.         0.        ]]\n",
      "0.91567546\n",
      "0.7817958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.08987249 0.         0.03070119 0.1768714  0.\n",
      "  0.         0.23009084 0.         0.14046697 0.07285628 0.\n",
      "  0.05590405 0.         0.         0.         0.         0.\n",
      "  0.         0.23182069 0.         0.18908818 0.13607123 0.\n",
      "  0.22047016 0.02299133 0.22002527 0.24567892 0.1936717  0.\n",
      "  0.         0.24855149 0.         0.08739811 0.         0.00591612\n",
      "  0.         0.12965688 0.         0.         0.         0.04613055\n",
      "  0.         0.         0.0054569  0.01055048 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14399378\n",
      "  0.         0.00902676 0.3958388  0.         0.         0.\n",
      "  0.04502255 0.         0.         0.01886963 0.         0.01163838\n",
      "  0.05150678 0.         0.05503293 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02899388 0.\n",
      "  0.         0.         0.         0.         0.         0.00577786\n",
      "  0.05800198 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09400591 0.         0.         0.05333159\n",
      "  0.14758675 0.04092914 0.         0.22962908 0.0328159  0.\n",
      "  0.         0.         0.         0.         0.257535   0.\n",
      "  0.         0.2388096  0.         0.         0.         0.\n",
      "  0.         0.18180287 0.         0.         0.16894631 0.\n",
      "  0.         0.         0.         0.         0.00618189 0.1438291\n",
      "  0.04553578 0.        ]]\n",
      "0.78261584\n",
      "0.93204933\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.15688182 0.         0.1464214  0.25773114 0.\n",
      "  0.         0.10962376 0.         0.         0.3116729  0.\n",
      "  0.         0.         0.00085027 0.         0.         0.03191967\n",
      "  0.         0.14288945 0.08107249 0.04948526 0.03385342 0.\n",
      "  0.07551229 0.         0.         0.         0.12977754 0.\n",
      "  0.         0.         0.         0.16284476 0.         0.12933274\n",
      "  0.         0.13280039 0.         0.         0.         0.\n",
      "  0.         0.00851787 0.0859881  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08950116 0.0432783\n",
      "  0.         0.         0.19009638 0.07994725 0.         0.08206249\n",
      "  0.28073975 0.03087867 0.         0.         0.         0.0125942\n",
      "  0.02755334 0.16213861 0.00563625 0.         0.         0.14765847\n",
      "  0.10156435 0.         0.         0.         0.12351096 0.\n",
      "  0.02676183 0.00318279 0.         0.         0.         0.0306547\n",
      "  0.13671738 0.         0.         0.15595852 0.         0.\n",
      "  0.         0.         0.         0.13913824 0.         0.\n",
      "  0.13888472 0.         0.06897181 0.         0.13874583 0.\n",
      "  0.         0.21329668 0.         0.         0.         0.\n",
      "  0.         0.26472926 0.         0.20058663 0.         0.0383453\n",
      "  0.         0.3011632  0.         0.09672365 0.2064432  0.\n",
      "  0.         0.03651284 0.03847203 0.08492775 0.         0.03798102\n",
      "  0.13882561 0.06564884]]\n",
      "0.8704643\n",
      "0.9226829\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.13804758 0.02666431 0.0885588  0.32634953 0.\n",
      "  0.         0.05481295 0.         0.10387585 0.23455003 0.\n",
      "  0.05834958 0.         0.         0.04851351 0.         0.\n",
      "  0.         0.12951617 0.00825818 0.         0.04848238 0.04477582\n",
      "  0.13536547 0.         0.0513872  0.22242866 0.2590026  0.\n",
      "  0.         0.14416303 0.         0.0357344  0.         0.\n",
      "  0.         0.1308323  0.         0.         0.         0.18747142\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02133594 0.16449083\n",
      "  0.         0.09419238 0.18338591 0.11364026 0.         0.1741046\n",
      "  0.07966033 0.1569227  0.         0.         0.         0.\n",
      "  0.04332511 0.         0.         0.         0.         0.\n",
      "  0.08277349 0.         0.         0.         0.14359558 0.\n",
      "  0.09526524 0.         0.         0.         0.         0.\n",
      "  0.06462887 0.         0.00472938 0.         0.         0.\n",
      "  0.         0.10617433 0.06306624 0.         0.         0.17345192\n",
      "  0.24985893 0.         0.         0.00372503 0.         0.02905266\n",
      "  0.         0.08625388 0.09713244 0.         0.21223159 0.\n",
      "  0.10024024 0.23404068 0.         0.         0.         0.\n",
      "  0.         0.15957598 0.         0.12999384 0.         0.0429341\n",
      "  0.         0.09562372 0.         0.         0.16098055 0.20978472\n",
      "  0.11315563 0.        ]]\n",
      "0.74566996\n",
      "0.9318609\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.2537487  0.         0.20325361 0.1265731  0.\n",
      "  0.         0.06974641 0.         0.         0.2729104  0.\n",
      "  0.0544199  0.04579989 0.03020638 0.         0.         0.14317673\n",
      "  0.         0.18137379 0.         0.10376646 0.00736924 0.\n",
      "  0.11890767 0.         0.         0.         0.06908192 0.\n",
      "  0.         0.         0.         0.         0.         0.14338562\n",
      "  0.         0.10573988 0.03799438 0.         0.         0.03707119\n",
      "  0.         0.05110149 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.12072238 0.11243647 0.14920989\n",
      "  0.         0.06518451 0.2482838  0.         0.         0.03321321\n",
      "  0.21314621 0.14198157 0.         0.         0.         0.\n",
      "  0.         0.1057435  0.         0.         0.00595904 0.07535785\n",
      "  0.25204018 0.         0.         0.         0.23489463 0.\n",
      "  0.         0.         0.         0.05603129 0.         0.\n",
      "  0.22747062 0.         0.         0.02863556 0.00781704 0.\n",
      "  0.         0.12154954 0.01067476 0.1316815  0.         0.\n",
      "  0.18966478 0.         0.09558509 0.04752787 0.         0.04835446\n",
      "  0.         0.11993122 0.13816981 0.06021277 0.19793408 0.\n",
      "  0.         0.13145857 0.         0.15884659 0.         0.\n",
      "  0.00575729 0.22591922 0.         0.12429902 0.13360582 0.\n",
      "  0.         0.         0.         0.         0.06676734 0.0005757\n",
      "  0.09692299 0.0169274 ]]\n",
      "0.9032989\n",
      "0.9092725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.11241665 0.15769427 0.00432189 0.17423011 0.\n",
      "  0.         0.         0.         0.         0.10505838 0.\n",
      "  0.         0.21046427 0.09701475 0.         0.12332974 0.\n",
      "  0.         0.31542954 0.04465519 0.         0.03952006 0.16495323\n",
      "  0.         0.         0.         0.24479876 0.03873619 0.07531048\n",
      "  0.         0.         0.         0.04248306 0.         0.06203818\n",
      "  0.         0.11862684 0.         0.         0.         0.2175964\n",
      "  0.05254328 0.08939525 0.18369247 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13079277 0.         0.         0.01046467\n",
      "  0.11750672 0.2902821  0.         0.         0.         0.0932264\n",
      "  0.         0.12935832 0.         0.0264788  0.         0.\n",
      "  0.09895001 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.02535605 0.         0.\n",
      "  0.11671134 0.06756288 0.         0.13939834 0.00278368 0.\n",
      "  0.10101946 0.         0.10624422 0.         0.04610511 0.\n",
      "  0.07433914 0.         0.13792883 0.         0.02866771 0.\n",
      "  0.         0.25896454 0.10305006 0.02367982 0.10460515 0.\n",
      "  0.         0.21632147 0.         0.         0.         0.11936771\n",
      "  0.         0.23971468 0.         0.         0.12383686 0.11072952\n",
      "  0.         0.1322495  0.         0.04328636 0.01617576 0.22328988\n",
      "  0.         0.        ]]\n",
      "0.8358626\n",
      "0.8840265\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.05359045 0.         0.19971271 0.09293785 0.\n",
      "  0.         0.17619899 0.         0.12276229 0.01158989 0.\n",
      "  0.1642689  0.         0.         0.         0.         0.02052508\n",
      "  0.         0.1712485  0.         0.1259787  0.10864962 0.00673577\n",
      "  0.13313371 0.         0.         0.19761382 0.16877887 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15067199 0.         0.01751545 0.         0.06416465\n",
      "  0.         0.01008259 0.11089277 0.         0.         0.\n",
      "  0.00260819 0.         0.         0.         0.07039156 0.11669619\n",
      "  0.         0.         0.0198301  0.07534957 0.         0.10581107\n",
      "  0.10826094 0.23587193 0.         0.         0.         0.14474519\n",
      "  0.03990385 0.24569774 0.05332239 0.         0.         0.02503274\n",
      "  0.06924947 0.         0.         0.         0.16375738 0.\n",
      "  0.         0.         0.10483756 0.03896459 0.         0.00103872\n",
      "  0.12617768 0.         0.         0.15033664 0.07608878 0.\n",
      "  0.         0.         0.04173323 0.         0.         0.02027378\n",
      "  0.         0.         0.         0.20448172 0.15321477 0.\n",
      "  0.         0.24219424 0.         0.         0.01853731 0.\n",
      "  0.         0.18942395 0.05272972 0.         0.         0.\n",
      "  0.         0.16129577 0.05647558 0.         0.22768675 0.15824004\n",
      "  0.         0.13292734 0.17059036 0.10030711 0.         0.23751172\n",
      "  0.17510988 0.        ]]\n",
      "0.8511891\n",
      "1.0188837\n",
      "[[0.         0.         0.1791362  0.         0.2050907  0.\n",
      "  0.         0.         0.         0.11244351 0.04214982 0.\n",
      "  0.         0.05628728 0.         0.         0.         0.\n",
      "  0.         0.07316411 0.         0.06948267 0.04043263 0.\n",
      "  0.36336276 0.026951   0.01040663 0.18900341 0.15315525 0.\n",
      "  0.         0.12265346 0.         0.         0.         0.08470065\n",
      "  0.         0.01954316 0.         0.         0.         0.14315897\n",
      "  0.         0.02183079 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11334564 0.09859664\n",
      "  0.         0.03922539 0.32190973 0.00375991 0.         0.07518752\n",
      "  0.2181036  0.17689864 0.         0.         0.         0.01329537\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.16289209 0.         0.         0.         0.         0.\n",
      "  0.         0.00780586 0.         0.         0.         0.\n",
      "  0.20365866 0.         0.         0.10193584 0.         0.\n",
      "  0.         0.         0.09463133 0.         0.         0.05230603\n",
      "  0.2150619  0.         0.         0.08963756 0.12140225 0.\n",
      "  0.         0.09725467 0.07811243 0.         0.0336116  0.\n",
      "  0.         0.25624222 0.         0.         0.         0.\n",
      "  0.         0.20556237 0.         0.         0.11893277 0.\n",
      "  0.         0.17316481 0.         0.17881027 0.         0.27454624\n",
      "  0.11455721 0.        ]]\n",
      "[[0.         0.18826778 0.09931731 0.         0.19902033 0.\n",
      "  0.         0.07259142 0.         0.03435858 0.00204392 0.\n",
      "  0.         0.         0.         0.         0.         0.07314678\n",
      "  0.         0.1956443  0.10277999 0.11595597 0.         0.\n",
      "  0.08969536 0.         0.01977132 0.08038978 0.16505438 0.\n",
      "  0.         0.         0.         0.26760548 0.         0.01886074\n",
      "  0.         0.15270634 0.         0.         0.         0.\n",
      "  0.         0.         0.01888521 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.15825288 0.07595807\n",
      "  0.         0.02682878 0.18796109 0.15299928 0.         0.20337489\n",
      "  0.304782   0.04112374 0.         0.         0.         0.13826758\n",
      "  0.         0.         0.         0.06589665 0.         0.\n",
      "  0.12547688 0.         0.         0.         0.11451785 0.\n",
      "  0.11566667 0.         0.         0.07059661 0.         0.\n",
      "  0.21494782 0.         0.         0.         0.         0.\n",
      "  0.         0.01610852 0.11646396 0.         0.         0.09127239\n",
      "  0.06730396 0.         0.         0.3122447  0.01537459 0.\n",
      "  0.         0.18354364 0.         0.01887049 0.07274951 0.\n",
      "  0.         0.20037691 0.         0.         0.         0.\n",
      "  0.         0.20998539 0.         0.         0.08952683 0.21027894\n",
      "  0.         0.04971442 0.         0.0358188  0.0844241  0.18919894\n",
      "  0.11487304 0.        ]]\n",
      "0.8110099\n",
      "0.8997665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2065d7f6e631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     anchor_encoding = encoding_network.predict([encoding_net_anchor_inputs], \n\u001b[1;32m     16\u001b[0m                                                \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                verbose = 0)   \n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mencoding_net_pos_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read one line at at time. Change chunksize to process more lines at a time. \n",
    "reader = pd.read_csv('../dataset/train/train1.csv', chunksize=1)\n",
    "write_header = True  # Needed to get header for first chunk\n",
    "\n",
    "for chunk in reader:\n",
    "    encoding_net_anchor_inputs = np.empty((0, 224, 224, 3))\n",
    "    anchor_img = image.load_img(chunk.iloc[0, 1], target_size=(224, 224))\n",
    "    anchor_img = image.img_to_array(anchor_img)\n",
    "    #print(anchor_imgs.shape)\n",
    "    anchor_img = np.expand_dims(anchor_img, axis=0)\n",
    "    #print(anchor_imgs.shape)\n",
    "    anchor_img = preprocess_input(anchor_img)\n",
    "    encoding_net_anchor_inputs = np.append(encoding_net_anchor_inputs, anchor_img, axis=0)\n",
    "\n",
    "    anchor_encoding = encoding_network.predict([encoding_net_anchor_inputs], \n",
    "                                               batch_size = 1, \n",
    "                                               verbose = 0)   \n",
    "\n",
    "    encoding_net_pos_inputs = np.empty((0, 224, 224, 3))\n",
    "    pos_img = image.load_img(chunk.iloc[0, 2], target_size=(224, 224))\n",
    "    pos_img = image.img_to_array(pos_img)\n",
    "    pos_img = np.expand_dims(pos_img, axis=0)\n",
    "    pos_img = preprocess_input(pos_img)\n",
    "    encoding_net_pos_inputs = np.append(encoding_net_pos_inputs, pos_img, axis=0)\n",
    "\n",
    "\n",
    "    pos_encoding = encoding_network.predict([encoding_net_pos_inputs], \n",
    "                                            batch_size = 1, \n",
    "                                            verbose = 0)\n",
    "\n",
    "    encoding_net_neg_inputs = np.empty((0, 224, 224, 3))\n",
    "    neg_img = image.load_img(chunk.iloc[0, 3], target_size=(224, 224))\n",
    "    neg_img = image.img_to_array(neg_img)\n",
    "    neg_img = np.expand_dims(neg_img, axis=0)\n",
    "    neg_img = preprocess_input(neg_img)\n",
    "    encoding_net_neg_inputs = np.append(encoding_net_neg_inputs, neg_img, axis=0)\n",
    "\n",
    "\n",
    "    neg_encoding = encoding_network.predict([encoding_net_neg_inputs], \n",
    "                                            batch_size = 1, \n",
    "                                            verbose = 0)\n",
    "\n",
    "    #positive_distance, negative_distance = calc_distances([anchor_encoding, pos_encoding, neg_encoding])\n",
    "    print(anchor_encoding)\n",
    "    print(pos_encoding)\n",
    "    positive_distance = np.linalg.norm(anchor_encoding - pos_encoding)\n",
    "    negative_distance = np.linalg.norm(anchor_encoding - neg_encoding)\n",
    "    print(positive_distance)\n",
    "    print(negative_distance)\n",
    "    chunk['pos_dist'] = positive_distance\n",
    "    chunk['neg_dist'] = negative_distance\n",
    "\n",
    "    # Save the file to a csv, appending each new chunk you process. mode='a' means append.\n",
    "    chunk.to_csv('train1_distances.csv', mode='a', header=write_header, index=False)\n",
    "    write_header = False  # Update so later chunks don't write header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one line at at time. Change chunksize to process more lines at a time. \n",
    "reader = pd.read_csv('../dataset/train/train1.csv', chunksize=2)\n",
    "write_header = True  # Needed to get header for first chunk\n",
    "i= 0\n",
    "for chunk in reader:\n",
    "    if i < 3:\n",
    "        print(i)\n",
    "        list_pos = list(chunk['pos_img'])\n",
    "        print(list_pos)\n",
    "        #print(chunk.iloc[0,2])\n",
    "        i += 1\n",
    "    # Do some stuff\n",
    "    #chunk['pos_dist'] = positive_distance\n",
    "    #chunk['neg_dist'] = negative_distance\n",
    "\n",
    "    # Save the file to a csv, appending each new chunk you process. mode='a' means append.\n",
    "    #chunk.to_csv('train1_distances.csv', mode='a', header=write_header, index=False)\n",
    "    #write_header = False  # Update so later chunks don't write header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train1_distances = \"train1_distances.csv\"\n",
    "column_names = ['ID', 'anchor_img', 'pos_img', 'neg_img', 'pos_dist', 'neg_dist']\n",
    "raw_data = {column_names[0]: [], \n",
    "            column_names[1]: [],\n",
    "            column_names[2]: [],\n",
    "            column_names[3]: [], \n",
    "            column_names[4]: [],\n",
    "            column_names[5]: []}\n",
    "df = pd.DataFrame(raw_data, columns = column_names)\n",
    "df.to_csv(train1_distances, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "i:  1\n",
      "i:  2\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for df in pd.read_csv('../dataset/train/train1.csv', chunksize=1):\n",
    "    if i < 3:\n",
    "        print(\"i: \", i)\n",
    "        df['pos_dist'] = 11\n",
    "        df['neg_dist'] = 22\n",
    "        df.to_csv(train1_distances, header=False, index=False, mode='a')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-03e17e063724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mcolumn_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                         column_names[5]: 22}\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mdf_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mdf_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;31m# GH10856\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;31m# raise ValueError if only scalars in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   7377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7379\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m   7380\u001b[0m                              ' an index')\n\u001b[1;32m   7381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "with open(train1_distances, 'a') as file:\n",
    "    i = 0\n",
    "    for df in pd.read_csv('../dataset/train/train1.csv', chunksize=1):\n",
    "        if i < 3:\n",
    "            print(\"i: \", i)\n",
    "            raw_data = {column_names[0]: df.iloc[0, 0], \n",
    "                        column_names[1]: df.iloc[0, 1],\n",
    "                        column_names[2]: df.iloc[0, 2],\n",
    "                        column_names[3]: df.iloc[0, 3],\n",
    "                        column_names[4]: 11,\n",
    "                        column_names[5]: 22}\n",
    "            df_dist = pd.DataFrame(raw_data, columns = column_names)\n",
    "            df_dist.to_csv(file, header=False, index=False)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path1 = '/home/cesncn/Desktop/github_projects/face_recognition/dataset/train/train1.csv'\n",
    "TRAIN_INPUT_PATHS = [train_path1]\n",
    "\n",
    "RECORD_DEFAULTS_TRAIN = [[0], [''], [''], ['']]\n",
    "\n",
    "def decode_csv_train(line):\n",
    "   parsed_line = tf.decode_csv(line, RECORD_DEFAULTS_TRAIN)\n",
    "   anchor_path = parsed_line[1]\n",
    "   pos_path  = parsed_line[2]\n",
    "   neg_path    = parsed_line[3]\n",
    "   return anchor_path, pos_path, neg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True   Should be TRUE\n",
      "False   Should be FALSE\n",
      "\n",
      "nr_epoch:  0 \n",
      "\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 325s 5s/step - loss: 1.8681\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 357s 6s/step - loss: 1.8398\n",
      "Epoch 1/1\n",
      "57/57 [==============================] - 320s 6s/step - loss: 1.7692\n",
      "Out of range error triggered (looped through training set).\n"
     ]
    }
   ],
   "source": [
    "iterator_batch_size = 64\n",
    "train_batch_size = 8\n",
    "\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv_train))\n",
    "dataset = dataset.shuffle(buffer_size=100000)\n",
    "dataset = dataset.batch(iterator_batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "train_eval_score = 0\n",
    "\n",
    "# Use the agnostic tensorflow session from Keras\n",
    "sess = K.get_session()    \n",
    "\n",
    "#print(\"bn3d_branch2c: (Should be same)\\n\", \n",
    "#      siamese_network.get_layer('model_1').get_layer('bn3d_branch2c').get_weights())\n",
    "#print(\"res3d_branch2c: (Should be same)\\n\", \n",
    "#      siamese_network.get_layer('model_1').get_layer('res3d_branch2c').get_weights())\n",
    "print(siamese_network.get_layer('model_1').get_layer('bn3d_branch2c').trainable, \"  Should be TRUE\")\n",
    "print(siamese_network.get_layer('model_1').get_layer('res3d_branch2c').trainable, \"  Should be FALSE\")\n",
    "\n",
    "nr_epochs = 1   # 18 epochs was enough to bring down the loss to zero for 8 unique faces\n",
    "for i in range(0, nr_epochs):\n",
    "    print(\"\\nnr_epoch: \", str(i), \"\\n\")\n",
    "    sess.run(iterator.initializer, feed_dict={filenames: TRAIN_INPUT_PATHS})\n",
    "    while True:\n",
    "        try:\n",
    "          anchor_path, pos_path, neg_path = sess.run(next_element)\n",
    "\n",
    "          anchor_imgs = np.empty((0, 224, 224, 3))\n",
    "          pos_imgs = np.empty((0, 224, 224, 3))\n",
    "          neg_imgs = np.empty((0, 224, 224, 3))\n",
    "          for j in range (0, len(anchor_path)):\n",
    "              #print(anchor_path)\n",
    "              anchor_img = image.load_img(anchor_path[j], target_size=(224, 224))\n",
    "              anchor_img = image.img_to_array(anchor_img)\n",
    "              #print(anchor_imgs.shape)\n",
    "              anchor_img = np.expand_dims(anchor_img, axis=0)\n",
    "              #print(anchor_imgs.shape)\n",
    "              anchor_img = preprocess_input(anchor_img)\n",
    "              anchor_imgs = np.append(anchor_imgs, anchor_img, axis=0)\n",
    "              #print(anchor_img.shape)\n",
    "\n",
    "              #print(test_path)\n",
    "              pos_img = image.load_img(pos_path[j], target_size=(224, 224))\n",
    "              pos_img = image.img_to_array(pos_img)\n",
    "              pos_img = np.expand_dims(pos_img, axis=0)\n",
    "              pos_img = preprocess_input(pos_img)\n",
    "              pos_imgs = np.append(pos_imgs, pos_img, axis=0)\n",
    "              #print(pos_img.shape)\n",
    "\n",
    "              neg_img = image.load_img(neg_path[j], target_size=(224, 224))\n",
    "              neg_img = image.img_to_array(neg_img)\n",
    "              neg_img = np.expand_dims(neg_img, axis=0)\n",
    "              neg_img = preprocess_input(neg_img)\n",
    "              neg_imgs = np.append(neg_imgs, neg_img, axis=0)\n",
    "              #print(neg_img.shape)\n",
    "\n",
    "          #print(\"len(anchor_imgs): \", len(anchor_imgs))\n",
    "          #print(\"pos_imgs[0].shape: \", pos_imgs[0].shape)\n",
    "          #print(\"neg_imgs.shape: \", neg_imgs.shape)\n",
    "\n",
    "          #print(labels)\n",
    "\n",
    "          # dummy output, needed for being able to run the fit(..) function\n",
    "          z = np.zeros(len(anchor_path))\n",
    "\n",
    "          siamese_network.fit(x=[anchor_imgs, pos_imgs, neg_imgs], \n",
    "                              y=z, \n",
    "                              batch_size=train_batch_size, \n",
    "                              epochs=1, \n",
    "                              verbose=1, \n",
    "                              callbacks=None, \n",
    "                              validation_split=0.0, \n",
    "                              validation_data=None, \n",
    "                              shuffle=True, \n",
    "                              class_weight=None, \n",
    "                              sample_weight=None, \n",
    "                              initial_epoch=0, \n",
    "                              steps_per_epoch=None, \n",
    "                              validation_steps=None)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "          print(\"Out of range error triggered (looped through training set).\")\n",
    "          break\n",
    "\n",
    "# Training completed at this point. Save the model architecture and weights.            \n",
    "            \n",
    "# Save the Siamese Network architecture\n",
    "siamese_model_json = siamese_network.to_json()\n",
    "with open(\"saved_model/siamese_network_arch.json\", \"w\") as json_file:\n",
    "    json_file.write(siamese_model_json)\n",
    "# save the Siamese Network model weights\n",
    "siamese_network.save_weights('saved_model/siamese_model_weights.h5')\n",
    "\n",
    "# create and save the Encoding Network to use in predictions later on\n",
    "encoding_input = Input(shape=(224, 224, 3), name='encoding_input')\n",
    "encoding_output   = new_model(encoding_input)\n",
    "encoding_network = Model(inputs  = encoding_input, \n",
    "                         outputs = encoding_output)\n",
    "\n",
    "weights = siamese_network.get_layer('model_1').get_weights()\n",
    "encoding_network.get_layer('model_1').set_weights(weights)\n",
    "\n",
    "# Save the Encoding Network architecture\n",
    "encoding_model_json = encoding_network.to_json()\n",
    "with open(\"saved_model/encoding_network_arch.json\", \"w\") as json_file:\n",
    "    json_file.write(encoding_model_json)\n",
    "# save the Encoding Network model weights    \n",
    "encoding_network.save_weights('saved_model/encoding_network_weights.h5')\n",
    "\n",
    "#print(\"bn3d_branch2c: (Should be different)\\n\", \n",
    "#      siamese_network.get_layer('model_1').get_layer('bn3d_branch2c').get_weights())\n",
    "#print(\"res3d_branch2c: (Should be same)\\n\", \n",
    "#      siamese_network.get_layer('model_1').get_layer('res3d_branch2c').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"bn3d_branch2c: (siamese)\\n\", \n",
    "#      siamese_network.get_layer('model_1').get_layer('bn3d_branch2c').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"bn3d_branch2c: (encoding)\\n\", \n",
    "#      encoding_network.get_layer('model_1').get_layer('bn3d_branch2c').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW RUN EVALUATION ON THE DEV SET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dev_path1 = '/home/cesncn/Desktop/github_projects/face_recognition/dataset/dev/dev1.csv'\n",
    "DEV_INPUT_PATHS = [dev_path1]\n",
    "\n",
    "RECORD_DEFAULTS_DEV = [[0], [''], [''], ['']]\n",
    "\n",
    "def decode_csv_dev(line):\n",
    "   parsed_line = tf.decode_csv(line, RECORD_DEFAULTS_DEV)\n",
    "   anchor_path = parsed_line[1]\n",
    "   pos_path  = parsed_line[2]\n",
    "   neg_path    = parsed_line[3]\n",
    "   return anchor_path, pos_path, neg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(anchor_imgs):  21\n",
      "neg_imgs.shape:  (21, 224, 224, 3)\n",
      "21/21 [==============================] - 37s 2s/step\n",
      "Evaluate Score (Loss):  1.8155710697174072\n",
      "Dev Set Evaluate Score (Loss):  1.8155710697174072\n",
      "len(anchor_imgs):  21\n",
      "neg_imgs.shape:  (21, 224, 224, 3)\n",
      "21/21 [==============================] - 40s 2s/step\n",
      "Evaluate Score (Loss):  1.7451629638671875\n",
      "Dev Set Evaluate Score (Loss):  1.7803670167922974\n",
      "len(anchor_imgs):  21\n",
      "neg_imgs.shape:  (21, 224, 224, 3)\n",
      "21/21 [==============================] - 33s 2s/step\n",
      "Evaluate Score (Loss):  1.7538011074066162\n",
      "Dev Set Evaluate Score (Loss):  1.771511713663737\n",
      "len(anchor_imgs):  21\n",
      "neg_imgs.shape:  (21, 224, 224, 3)\n",
      "21/21 [==============================] - 33s 2s/step\n",
      "Evaluate Score (Loss):  1.755384922027588\n",
      "Dev Set Evaluate Score (Loss):  1.7674800157546997\n",
      "Out of range error triggered (looped through training set)\n",
      "Dev Set Evaluate Score (Loss):  1.755384922027588\n"
     ]
    }
   ],
   "source": [
    "batch_size = 21\n",
    "\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "dataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv_dev))\n",
    "dataset = dataset.shuffle(buffer_size=100000)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "dev_eval_score = 0\n",
    "nr_dev_examples = 0\n",
    "sess.run(iterator.initializer, feed_dict={filenames: DEV_INPUT_PATHS})\n",
    "while True:\n",
    "    try:\n",
    "      anchor_path, pos_path, neg_path = sess.run(next_element)\n",
    "\n",
    "      anchor_imgs = np.empty((0, 224, 224, 3))\n",
    "      pos_imgs = np.empty((0, 224, 224, 3))\n",
    "      neg_imgs = np.empty((0, 224, 224, 3))\n",
    "      for j in range (0, len(anchor_path)):\n",
    "          #print(anchor_path)\n",
    "          anchor_img = image.load_img(anchor_path[j], target_size=(224, 224))\n",
    "          anchor_img = image.img_to_array(anchor_img)\n",
    "          #print(anchor_imgs.shape)\n",
    "          anchor_img = np.expand_dims(anchor_img, axis=0)\n",
    "          #print(anchor_imgs.shape)\n",
    "          anchor_img = preprocess_input(anchor_img)\n",
    "          anchor_imgs = np.append(anchor_imgs, anchor_img, axis=0)\n",
    "          #print(anchor_img.shape)\n",
    "\n",
    "          #print(test_path)\n",
    "          pos_img = image.load_img(pos_path[j], target_size=(224, 224))\n",
    "          pos_img = image.img_to_array(pos_img)\n",
    "          pos_img = np.expand_dims(pos_img, axis=0)\n",
    "          pos_img = preprocess_input(pos_img)\n",
    "          pos_imgs = np.append(pos_imgs, pos_img, axis=0)\n",
    "          #print(pos_img.shape)\n",
    "\n",
    "          neg_img = image.load_img(neg_path[j], target_size=(224, 224))\n",
    "          neg_img = image.img_to_array(neg_img)\n",
    "          neg_img = np.expand_dims(neg_img, axis=0)\n",
    "          neg_img = preprocess_input(neg_img)\n",
    "          neg_imgs = np.append(neg_imgs, neg_img, axis=0)\n",
    "          #print(neg_img.shape)\n",
    "\n",
    "      print(\"len(anchor_imgs): \", len(anchor_imgs))\n",
    "      #print(\"pos_imgs[0].shape: \", pos_imgs[0].shape)\n",
    "      print(\"neg_imgs.shape: \", neg_imgs.shape)\n",
    "\n",
    "      #print(labels)\n",
    "\n",
    "      # dummy output, needed for being able to run the fit(..) function\n",
    "      z = np.zeros(len(anchor_path))\n",
    "\n",
    "      eval_score = siamese_network.evaluate(x=[anchor_imgs, pos_imgs, neg_imgs], \n",
    "                                            y=z,\n",
    "                                            batch_size = batch_size, \n",
    "                                            verbose = 1)\n",
    "\n",
    "      print(\"Evaluate Score (Loss): \", eval_score)\n",
    "      dev_eval_score += (eval_score * len(anchor_imgs))\n",
    "      nr_dev_examples += len(anchor_imgs)\n",
    "      print(\"Dev Set Evaluate Score (Loss): \", dev_eval_score/nr_dev_examples)\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"Out of range error triggered (looped through training set)\")\n",
    "      break\n",
    "\n",
    "#dev_eval_score /= nr_dev_examples\n",
    "print(\"Dev Set Evaluate Score (Loss): \", eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
